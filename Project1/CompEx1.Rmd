---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 1: Group 13"
author: "Erik Andrè Klepp Vik and Vemund Tjessem"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  #html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

```{r,eval=TRUE,echo=FALSE}
# install.packages("knitr") #probably already installed
# install.packages("rmarkdown") #probably already installed
# install.packages("ggplot2") #plotting with ggplot
# install.packages("ggfortify")  
# install.packages("MASS")  
# install.packages("dplyr")  
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
```


# Problem 1

For this problem you will need to include some LaTex code. Please install latex on your computer and then consult Compulsor1.Rmd for hints how to write formulas in LaTex. $\LaTeX$

## a) 

The expression for the test mean squared error is
\begin{equation}
\text{MSE}_{\text{test}}=\frac{1}{n_0}\sum_{j=1}^{n_0} (y_{0j}-\hat{f}(x_{0j}))^2
\end{equation}

The expected test mean squared error (MSE) at $x_0$ is defined as: 

\begin{equation}
\text{E}[y_0 - \hat{f}(x_0)]^2
\end{equation}

## b)

Using the fact that $y_0=f(x_0)+\varepsilon$ gives

\begin{equation}
\text{E}[y_0 - \hat{f}(x_0)]^2 =  \underbrace{\text{Var}(\varepsilon)}_{\text{Irreducible error}} + \underbrace{\text{Var}(\hat{f}(x_0))}_{\text{Variance of prediction}} + \underbrace{\left( f(x_0) - \text{E}[\hat{f}(x_0)] \right)^2}_{\text{Squared bias}}
\end{equation}

## c)

## d)

## e)

## f)

## g)

# Problem 2

Here is a code chunk:

```{r, eval=TRUE}
id <- "1nLen1ckdnX4P9n8ShZeU7zbXpLc7qiwt" # google file ID
d.worm <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
head(d.worm)
```




## a)


## b) 
Below you have to complete the code and then replace `eval=FALSE` by `eval=TRUE` in the chunk options:
```{r, eval=FALSE, echo=TRUE}
ggplot(d.worm,aes(x= ... ,y=  ... ,colour= ...)) + geom_point() + theme_bw()
```

Note that the default figure width and height have been set globally as `fig.width=4, fig.height=3`, but if you would like to change that (e.g., due to space constraints), you can include a different width and height directly into the chunk options, again using `fig.width=..., fig.height=...`.

## c)

## d)

## e)

## f)

## g)


# Problem 3

Loading the files:
```{r, echo=TRUE, eval=TRUE}
id <- "1GNbIhjdhuwPOBr0Qz82JMkdjUVBuSoZd"
tennis <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", 
    id), header = T)
head(tennis)
```

## a)
We have
\begin{equation}
\label{eq:p_i}
p_i = \frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \beta_3x_{i3} + \beta_4 x_{i4}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}} 
\end{equation}

which gives

\begin{equation}
\label{eq:logit}
  \text{logit}(p_i) = \log\left(\frac{p_i}{1-p_i}\right)= \log\left(\frac{\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \beta_3x_{i3} + \beta_4 x_{i4}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}}}{1-\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \beta_3x_{i3} + \beta_4 x_{i4}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}}}\right)
\end{equation}

Multiplying both the numerator and denominator in Equation \ref{eq:logit} by $1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}$ gives 

\begin{equation}
  \text{logit}(p_i) = \log\left( \frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}}{1 + e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}} - (e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}})} \right)
\end{equation}

which further results in 

\begin{equation}
\label{eq:linear}
  \text{logit}(p_i) = \log(e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}
\end{equation}

It can be seen from Equation \ref{eq:linear} that $\text{logit}(p_i)$ is a linear function of the covariates $x_{i1}$, $x_{i2}$, $x_{i3}$ and $x_{i4}$.

## b)

```{r, echo=TRUE, eval=TRUE}
r.tennis = glm(Result ~ ACE.1 + ACE.2 + UFE.1 + UFE.2, data = tennis, family = "binomial")
summary(r.tennis)
```

Since the $\beta_1$ is positive one more ace for player one would increase the probability of player 1 winning. By incerasing $x_i$ by 1 the odds ratio for $Y_i=1$ increases by $\exp(\beta_1)$.

## c)

```{r, echo=TRUE, eval=TRUE}
# make variables for difference
tennis$ACEdiff = tennis$ACE.1 - tennis$ACE.2
tennis$UFEdiff = tennis$UFE.1 - tennis$UFE.2

# divide into test and train set
n = dim(tennis)[1]
n2 = n/2
set.seed(1234)  # to reproduce the same test and train sets each time you run the code
train = sample(c(1:n), replace = F)[1:n2]
tennisTest = tennis[-train, ]
tennisTrain = tennis[train, ]
```

The code for fitting a logistic regression model is given below. 

```{r tennisTrain, echo=TRUE, eval=TRUE}
# Fitting a logistic regression model on the form Result ∼ ACEdiff + UFEdiff on the training set
fit.3c = glm(Result ~ ACEdiff + UFEdiff, data = tennisTrain, family = "binomial")
summary(fit.3c)
```

The class boundary will be where $\hat P(Y=1|\boldsymbol{x}) =0.5$. When the probability is 0.5 we have $\text{logit}(p_i)=\log(1)=0$

\begin{equation}
  0 = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2}
\end{equation}

This gives the class boundary

\begin{equation}
  x_{2} = - \frac{\beta_0}{\beta2} - \frac{\beta_1}{\beta_2}x_{1}
\end{equation}



```{r, eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center"}
cof = coef(fit.3c)
a = - cof[1]/cof[3]
b = -cof[2]/cof[3]
```
The class boundary will be $x_2 = `r round(a, 3)` + `r round(b, 3)`x_1$

Making a plot of the training observations and the class boundary.
```{r , eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center"}
df = data.frame(tennisTrain, "Player1won"=as.factor(tennisTrain$Result))
ggplot(df, aes(x = ACEdiff, y = UFEdiff, color = Player1won)) + geom_abline(intercept=a, slope=b) + geom_point(size=1) + theme_bw()
```

Making a confusion matrix

```{r, echo=TRUE, eval=TRUE}
prd = predict(fit.3c, tennisTest, type="response")
confMat = table(tennisTest$Result, prd>0.5)
confMat
```

The sensitivity is `r confMat[2,2]/(confMat[2,1] + confMat[2,2])` and the specificity is `r round(confMat[1,1]/(confMat[1,1] + confMat[1,2]), 3)`


## d)

* $\pi_k$ is the prior class probabilities $\pi_k=\text{Pr}(Y=k)$. In this case it will be the probability for player 1 winning and for player 1 losing. 

* $\boldsymbol\mu_k$ is the mean of class $k$. In this case it will be a vector with the mean of difference in aces and difference in unforced errors. 

* $\boldsymbol \Sigma$ is the covariance matrix, and in this case it is assumed equal for both classes since LDA is used. The diagonal elements will be the variance of the difference in aces the variance of the difference in unforced errors and the off-diagonal elements will be the covariance.

* $f_k(\boldsymbol x)$ is the probability density function for $\boldsymbol X$ in class $k$, and is here assumed to be multivariate normally distributed with mean $\boldsymbol \mu_k$ and covariance $\boldsymbol \Sigma$.

## e)

### Part 1

\begin{equation}
P(Y=0 | {\bf X}={\boldsymbol x}) = P(Y=1 | {\bf X}={\boldsymbol x})
\end{equation}

The first step is to insert for the probability and the probability distribution.

\begin{equation}
\frac{\pi_0}{\sum_{l=1}^K \pi_l f_l({\boldsymbol x})2 \pi|\boldsymbol{\Sigma}|^{1/2}}e^{-\frac{1}{2}({\boldsymbol x}-\boldsymbol{\mu_0})^T \boldsymbol{\Sigma}^{-1}({\boldsymbol x}-\boldsymbol{\mu_0})} = \frac{\pi_1}{\sum_{l=1}^K \pi_l f_l({\boldsymbol x})2 \pi|\boldsymbol{\Sigma}|^{1/2}}e^{-\frac{1}{2}({\boldsymbol x}-\boldsymbol{\mu_1})^T \boldsymbol{\Sigma}^{-1}({\boldsymbol x}-\boldsymbol{\mu_1})}
\end{equation}

Next is taking the logarithm on both sides, which gives

\begin{align}
&\log(\pi_0)-\frac{1}{2}\boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol x + \boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0 + \boldsymbol \mu_0^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0 - \log\left(\sum_{l=1}^K \pi_l f_l({\boldsymbol x})2 \pi|\boldsymbol{\Sigma}|^{1/2}\right) = \\ &\log(\pi_1)-\frac{1}{2}\boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol x + \boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1 + \boldsymbol \mu_1^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1 - \log\left(\sum_{l=1}^K \pi_l f_l({\boldsymbol x})2 \pi|\boldsymbol{\Sigma}|^{1/2}\right)
\end{align}

After removing the terms not depending on $k$, which are equal on both sides

\begin{equation}
\label{eq:deltalikdelta}
\log(\pi_0) + \boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0 + \frac{1}{2} \boldsymbol \mu_0^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0 = \log(\pi_1) + \boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1 + \frac{1}{2} \boldsymbol \mu_1^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1
\end{equation}

which equals

\begin{equation}
\delta_0({\boldsymbol x}) = \delta_1({\boldsymbol x})
\end{equation}

### Part 2

The class boundary will be the values for $\boldsymbol x$ where $\delta_0({\boldsymbol x}) = \delta_1({\boldsymbol x})$, hence Equation \ref{eq:deltalikdelta} can be used to find the class boundary.


\begin{equation}
\boldsymbol x^T \boldsymbol \Sigma^{-1}  (\boldsymbol\mu_0 - \boldsymbol\mu_1) = \log\left(\frac{\pi_1}{\pi_0}\right) + \frac{1}{2} \boldsymbol \mu_1^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1 - \frac{1}{2} \boldsymbol \mu_0^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0
\end{equation}

Doing the matrix multiplications here will result in a equation on the form $ax_1 + bx_2=c$, which can be used to find the class boundary on the form $x_2 = \frac{c}{b} - \frac{a}{b}x_1$.

Since $\pi_k$, $\boldsymbol \mu_k$ and $\boldsymbol \Sigma$ are unknown, they have to be estimated. The estimators used are 

* $\hat{\pi}_k = \frac{n_k}{n}$
* $\hat{\boldsymbol{\mu}}_k = \frac{1}{n_k}\sum_{i:y_i=k} {\bf X}_i$
* $\hat{\boldsymbol{\Sigma}}_k=\frac{1}{n_k-1}\sum_{i:y_i=k} ({\bf X}_i-\hat{\boldsymbol{\mu}}_k ) ({\bf X}_i-\hat{\boldsymbol{\mu}}_k)^T$
* $\hat{\boldsymbol{\Sigma}}= \sum_{k=1}^K \frac{n_k - 1}{n - K} \cdot \hat{\boldsymbol{\Sigma}}_k$

```{r, eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center"}
k0s = subset(tennisTrain, tennisTrain$Result < 0.5)
k1s = subset(tennisTrain, tennisTrain$Result > 0.5)
n0 = length(k0s$UFEdiff)
n1 = length(k1s$UFEdiff)
pi0 = n0/(n0 + n1)
pi1 = n1/(n0 + n1)
mu0 = matrix(c(mean(k0s$ACEdiff), mean(k0s$UFEdiff)), nrow = 2)
mu1 = matrix(c(mean(k1s$ACEdiff), mean(k1s$UFEdiff)), nrow = 2)
mu0
mu1
covmat0 = cov(cbind(k0s$ACEdiff, k0s$UFEdiff))
covmat1 = cov(cbind(k1s$ACEdiff, k1s$UFEdiff))
covK = 1/(n0 + n1)*((n0-1) * covmat0 + (n1 - 1) * covmat1)
c = log(pi1/pi0) + 0.5 * t(mu1)%*%solve(covK)%*%mu1 - 0.5 * t(mu0)%*%solve(covK)%*%mu0
lhs = solve(covK)%*%(mu0-mu1)
a = lhs[1]
b = lhs[2]
```

Find that $a=`r round(a, 3)`$, $b=`r round(b,3)`$ and $c=`r round(c, 3)`$, which gives the class boundary $x_2 = `r round(c/b, 3)` + `r -round(a/b,3)` x_1$

### Part 3
```{r, eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center"}
tennis$istrain = 1
tennis[-train, ]$istrain = 0
df2 = data.frame(tennis, "Player1won"=as.factor(tennis$Result), "Istrain"=as.factor(tennis$istrain))
ggplot(df2, aes(x = ACEdiff, y = UFEdiff, color = Player1won, shape = Istrain)) + geom_abline(intercept=c/b, slope=-a/b) + geom_point(size=1) + theme_bw()
```

## f)

```{r, echo=TRUE, eval=TRUE}
lda.fit = lda(Result ~ ACEdiff + UFEdiff, data = tennisTrain)
lda.fit.p = predict(lda.fit, tennisTest)$class
confMat = table(lda.fit.p, tennisTest$Result)
confMat
```

The sensitivity is `r round(confMat[2,2]/(confMat[2,1] + confMat[2,2]), 3)` and the specificity is `r round(confMat[1,1]/(confMat[1,1] + confMat[1,2]), 3)`
  
## g)

```{r, echo=TRUE, eval=TRUE}
qda.fit = qda(Result ~ ACEdiff + UFEdiff, data = tennisTrain)
qda.fit.p = predict(qda.fit, tennisTest)$class
confMat = table(qda.fit.p, tennisTest$Result)
confMat
```

The sensitivity is `r round(confMat[2,2]/(confMat[2,1] + confMat[2,2]), 3)` and the specificity is `r round(confMat[1,1]/(confMat[1,1] + confMat[1,2]), 3)`

## h)

Looking at the confusion matrices, QDA has the lowest sensitivity and the specificity is lower than for LDA, so QDA should not be used. Looking at the plot of the QDA decision boundary we see that to the left in the plot around `UFEdiff`=0 and below QDA classifies as win while glm and LDA would classify as loss, this may be why QDA is worse than the other two. glm has the highest sensitivity and the lowest specificity while LDA has better sensitivity than QDA and the best spesificity. Summing up the two values glm has the highest value, so that is what should be used.


# Problem 4

## a)

Given a set of values for $K$, 10-fold cross validation is performed by first randomly dividing the data into a training set and a testing set, the testing set is not used until the very end. The training dataset is then randomly divided into 10 more or less equal parts, $C_1$, $C_2$, ..., $C_{10}$. $C_k$ denotes the indices of the observations in part $k$. 9 parts are used for training the model and 1 is used for testing the model. This is done 10 times with a new set used as test set each time. The error is then calculated using the loss function in Equation \ref{eq:10foldCV}.

\begin{equation}
\label{eq:10foldCV}
\text{CV}_{10} = \sum_{k=1}^{10} \frac{n_k}{10} \text{Err}_k
\end{equation}

where $n_k$ is the number of observations in part $k$. The error for part $k$ is

\begin{equation}
\text{Err}_k  = \sum_{i\in C_k} \frac{\text{I}(y_i\neq \hat y_i)}{n_k}
\end{equation}

where $\text{I}$ is the indicator function defined as 

\begin{equation}
\text{I}(a\neq\hat{a}) = \begin{cases} 1 \text{ if } a \neq \hat{a} \\ 
0 \text{ else } \end{cases}
\end{equation}

This is done for each value of $K$ we want to consider. This will result in a plot of  $\text{CV}_{10}$ against $K$. Based on this plot the best model can be selected. the best model will typically be the one with the lowest $\text{CV}_{10}$. The model is then fit using the whole training dataset, and tested using the test set which has not been used yet. 

## b)

(i) True
(ii) True
(iii) False
(iv) False

## c)


```{r, eval=TRUE, echo=TRUE}
id <- "1I6dk1fA4ujBjZPo3Xj8pIfnzIa94WKcy" # google file ID
d.chd <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
fit.4c = glm(chd ~ sbp + sex, data = d.chd, family = "binomial")
summary(fit.4c)
eta <- summary(fit.4c)$coef[, 1] %*% c(1, 140, 1)
pchd = exp(eta)/(1 + exp(eta))
```

The probability of a male with $sbp=140$ having coronary heart disease is `r round(pchd, 3)`.

```{r, eval=FALSE, echo=FALSE, fig.width=6,fig.height=4,fig.align="center"}
ggplot(d.chd, aes(x = sbp, y = sex, color=as.factor(chd))) + geom_point(size=1) + theme_bw()
```


## d)

Will now perform bootstrapping with $B=1000$. For each iteration the model is fit with the resampled data and is used to create a bootstrap estimate of the probability for `chd`, given `sbp`=140 and `sex`=male, $p$. The bootstrap estimates, $\hat{p}_i$, are stored in the `estimator` variable. 

```{r, eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center", cache=TRUE}
B = 1000
n = dim(d.chd)[1]
estimator = rep(NA, B)
for (b in 1:B) {
  newind = sample(x = c(1:n), size = n, replace = TRUE)
  newsample = d.chd[newind,1:3]
  fit.4d = glm(chd ~ sbp + sex, data = newsample, family = "binomial")
  eterm <- summary(fit.4d)$coef[, 1] %*% c(1, 140, 1)
  estimator[b] = exp(eterm)/(1 + exp(eterm))
}
Mxb = mean(estimator)
SE = sqrt(1/(B-1) * sum((estimator-Mxb)^2))
confinterval = quantile(estimator, probs = c(2.5, 97.5)/100)
```

Now that there are $1000$ different values for the probability the standard error can be calculated. The mean is simply

\begin{equation}
\bar{\hat{p}} = \frac{1}{B}\sum_{i=1}^B \hat{p}_{i}
\end{equation}

The standard error is then

\begin{equation}
\text{SE}_B(\hat{p}) = \sqrt{\frac{1}{B-1}\sum_{i=1}^B (\hat{p}_{i}-\bar{\hat{p}})^2} = `r SE`
\end{equation}


The standard error is `r round(SE, 4)`. The confidence interval for $\hat p$ is (`r round(confinterval, 3)`)






