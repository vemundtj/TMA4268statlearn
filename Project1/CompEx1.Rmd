---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 1: Group 13"
author: "Erik Andrè Klepp Vik and Vemund Tjessem"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document
  #pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

```{r,eval=TRUE,echo=FALSE}
# install.packages("knitr") #probably already installed
# install.packages("rmarkdown") #probably already installed
# install.packages("ggplot2") #plotting with ggplot
# install.packages("ggfortify")  
# install.packages("MASS")  
# install.packages("dplyr")  
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
```


# Problem 1

For this problem you will need to include some LaTex code. Please install latex on your computer and then consult Compulsor1.Rmd for hints how to write formulas in LaTex.

## a) 

The expression for the test mean squared error is
\begin{equation}
\text{MSE}_{\text{test}}=\frac{1}{n_0}\sum_{j=1}^{n_0} (y_{0j}-\hat{f}(x_{0j}))^2
\end{equation}

The expected test mean squared error (MSE) at $x_0$ is defined as: 

\begin{equation}
\text{E}[y_0 - \hat{f}(x_0)]^2
\end{equation}

## b)

Using the fact that $y_0=f(x_0)+\varepsilon$ gives

\begin{equation}
\text{E}[y_0 - \hat{f}(x_0)]^2 =  \underbrace{\text{Var}(\varepsilon)}_{\text{Irreducible error}} + \underbrace{\text{Var}(\hat{f}(x_0))}_{\text{Variance of prediction}} + \underbrace{\left( f(x_0) - \text{E}[\hat{f}(x_0)] \right)^2}_{\text{Squared bias}}
\end{equation}

## c)

## d)

## e)

## f)

## g)

# Problem 2

Here is a code chunk:

```{r, eval=TRUE}
id <- "1nLen1ckdnX4P9n8ShZeU7zbXpLc7qiwt" # google file ID
d.worm <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
head(d.worm)
```




## a)


## b) 
Below you have to complete the code and then replace `eval=FALSE` by `eval=TRUE` in the chunk options:
```{r, eval=FALSE, echo=TRUE}
ggplot(d.worm,aes(x= ... ,y=  ... ,colour= ...)) + geom_point() + theme_bw()
```

Note that the default figure width and height have been set globally as `fig.width=4, fig.height=3`, but if you would like to change that (e.g., due to space constraints), you can include a different width and height directly into the chunk options, again using `fig.width=..., fig.height=...`.

## c)

## d)

## e)

## f)

## g)


# Problem 3

Loading the files:
```{r, echo=TRUE, eval=TRUE}
id <- "1GNbIhjdhuwPOBr0Qz82JMkdjUVBuSoZd"
tennis <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", 
    id), header = T)
head(tennis)
```

## a)
We have
\begin{equation}
\label{eq:p_i}
p_i = \frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \beta_3x_{i3} + \beta_4 x_{i4}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}} 
\end{equation}

which gives

\begin{equation}
\label{eq:logit}
  \text{logit}(p_i) = \log\left(\frac{p_i}{1-p_i}\right)= \log\left(\frac{\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \beta_3x_{i3} + \beta_4 x_{i4}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}}}{1-\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \beta_3x_{i3} + \beta_4 x_{i4}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}}}\right)
\end{equation}

Multiplying both the numerator and denominator in Equation \ref{eq:logit} by $1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}$ gives 

\begin{equation}
  \text{logit}(p_i) = \log\left( \frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}}{1 + e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}} - (e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}})} \right)
\end{equation}

which further results in 

\begin{equation}
\label{eq:linear}
  \text{logit}(p_i) = \log(e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}
\end{equation}

It can be seen from Equation \ref{eq:linear} that $\text{logit}(p_i)$ is a linear function of the covariates $x_{i1}$, $x_{i2}$, $x_{i3}$ and $x_{i4}$.

## b)

```{r, echo=TRUE, eval=TRUE}
r.tennis = glm(Result ~ ACE.1 + ACE.2 + UFE.1 + UFE.2, data = tennis, family = "binomial")
summary(r.tennis)
```

Since the $\beta_1$ is positive one more ace for player one would increase the probability of player 1 winning. By incerasing $x_i$ by 1 the odds ratio for $Y_i=1$ increases by $\exp(\beta_1)$.

## c)

```{r, echo=TRUE, eval=TRUE}
# make variables for difference
tennis$ACEdiff = tennis$ACE.1 - tennis$ACE.2
tennis$UFEdiff = tennis$UFE.1 - tennis$UFE.2

# divide into test and train set
n = dim(tennis)[1]
n2 = n/2
set.seed(1234)  # to reproduce the same test and train sets each time you run the code
train = sample(c(1:n), replace = F)[1:n2]
tennisTest = tennis[-train, ]
tennisTrain = tennis[train, ]
```

The code for fitting a logistic regression model is given below. 

```{r tennisTrain, echo=TRUE, eval=TRUE}
# Fitting a logistic regression model on the form Result ∼ ACEdiff + UFEdiff on the training set
fit.3c = glm(Result ~ ACEdiff + UFEdiff, data = tennisTrain, family = "binomial")
summary(fit.3c)
```

The class boundary will be where $\hat P(Y=1|\boldsymbol{x}) =0.5$. When the probability is 0.5 we have $\text{logit}(p_i)=\log(1)=0$

\begin{equation}
  0 = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2}
\end{equation}

This gives the class boundary

\begin{equation}
  x_{2} = - \frac{\beta_0}{\beta2} - \frac{\beta_1}{\beta_2}x_{1}
\end{equation}



```{r, eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center"}
cof = coef(fit.3c)
a = - cof[1]/cof[3]
b = -cof[2]/cof[3]
```
The class boundary will be $x_2 = `r round(a, 3)` + `r round(b, 3)`x_1$

Making a plot of the training observations and the class boundary.
```{r , eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center"}
df = data.frame(tennisTrain, "Player1won"=as.factor(tennisTrain$Result))
ggplot(df, aes(x = ACEdiff, y = UFEdiff, color = Player1won)) + geom_abline(intercept=a, slope=b) + geom_point(size=1) + theme_bw()
```

Making a confusion matrix

```{r, echo=TRUE, eval=TRUE}
prd = predict(fit.3c, tennisTest, type="response")
confMat = table(tennisTest$Result, prd>0.5)
confMat
```

The sensitivity is `r confMat[2,2]/(confMat[2,1] + confMat[2,2])` and the specificity is `r round(confMat[1,1]/(confMat[1,1] + confMat[1,2]), 3)`


## d)

* $\pi_k$ is the prior class probabilities $\pi_k=\text{Pr}(Y=k)$. In this case it will be the probability for player 1 winning and for player 1 losing. 

* $\boldsymbol\mu_k$ is the mean of class $k$. In this case it will be a vector with the mean of difference in aces and difference in unforced errors. 

* $\boldsymbol \Sigma$ is the covariance matrix, and in this case it is assumed equal for both classes. The diagonal elements will be the variance of the difference in aces the variance of the difference in unforced errors and the off-diagonal elements will be the covariance.

* $f_k(\boldsymbol x)$ is the probability distribution of class $k$, and is assumed to be multivariate normally distributed with mean $\boldsymbol \mu_k$ and covariance $\boldsymbol \Sigma$.

## e)

### Part 1

\begin{equation}
P(Y=0 | {\bf X}={\boldsymbol x}) = P(Y=1 | {\bf X}={\boldsymbol x})
\end{equation}

The first step is to insert for the probability and the probability distribution.

\begin{equation}
\frac{\pi_0}{\sum_{l=1}^K \pi_l f_l({\boldsymbol x})2 \pi|\boldsymbol{\Sigma}|^{1/2}}e^{-\frac{1}{2}({\boldsymbol x}-\boldsymbol{\mu_0})^T \boldsymbol{\Sigma}^{-1}({\boldsymbol x}-\boldsymbol{\mu_0})} = \frac{\pi_1}{\sum_{l=1}^K \pi_l f_l({\boldsymbol x})2 \pi|\boldsymbol{\Sigma}|^{1/2}}e^{-\frac{1}{2}({\boldsymbol x}-\boldsymbol{\mu_1})^T \boldsymbol{\Sigma}^{-1}({\boldsymbol x}-\boldsymbol{\mu_1})}
\end{equation}

Next is taking the logarithm on both sides, which gives

\begin{align}
&\log(\pi_0)-\frac{1}{2}\boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol x + \boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0 + \boldsymbol \mu_0^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0 - \log\left(\sum_{l=1}^K \pi_l f_l({\boldsymbol x})2 \pi|\boldsymbol{\Sigma}|^{1/2}\right) = \\ &\log(\pi_1)-\frac{1}{2}\boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol x + \boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1 + \boldsymbol \mu_1^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1 - \log\left(\sum_{l=1}^K \pi_l f_l({\boldsymbol x})2 \pi|\boldsymbol{\Sigma}|^{1/2}\right)
\end{align}

After removing the terms not depending on $k$, which are equal on both sides

\begin{equation}
\label{eq:deltalikdelta}
\log(\pi_0) + \boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0 + \frac{1}{2} \boldsymbol \mu_0^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0 = \log(\pi_1) + \boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1 + \frac{1}{2} \boldsymbol \mu_1^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1
\end{equation}

which equals

\begin{equation}
\delta_0({\boldsymbol x}) = \delta_1({\boldsymbol x})
\end{equation}

### Part 2

The class boundary will be the values for $\boldsymbol x$ where $\delta_0({\boldsymbol x}) = \delta_1({\boldsymbol x})$, hence Equation \ref{eq:deltalikdelta} can be used to find the class boundary.


\begin{equation}
\boldsymbol x^T \boldsymbol \Sigma^{-1}  (\boldsymbol\mu_0 - \boldsymbol\mu_1) = \log\left(\frac{\pi_1}{\pi_0}\right) + \frac{1}{2} \boldsymbol \mu_1^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1 - \frac{1}{2} \boldsymbol \mu_0^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0
\end{equation}

Doing the matrix multiplications here will result in a equation on the form $ax_1 + bx_2=c$, which can be used to find the class boundary on the form $x_2 = \frac{c}{b} - \frac{a}{b}x_1$.

Since $\pi_k$, $\boldsymbol \mu_k$ and $\boldsymbol \Sigma$ are unknown, they have to be estimated. The estimators used are 

* $\hat{\pi}_k = \frac{n_k}{n}$
* $\hat{\boldsymbol{\mu}}_k = \frac{1}{n_k}\sum_{i:y_i=k} {\bf X}_i$
* $\hat{\boldsymbol{\Sigma}}_k=\frac{1}{n_k-1}\sum_{i:y_i=k} ({\bf X}_i-\hat{\boldsymbol{\mu}}_k ) ({\bf X}_i-\hat{\boldsymbol{\mu}}_k)^T$
* $\hat{\boldsymbol{\Sigma}}= \sum_{k=1}^K \frac{n_k - 1}{n - K} \cdot \hat{\boldsymbol{\Sigma}}_k$

```{r, eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center"}
k0s = subset(tennisTrain, tennisTrain$Result < 0.5)
k1s = subset(tennisTrain, tennisTrain$Result > 0.5)
n0 = length(k0s$UFEdiff)
n1 = length(k1s$UFEdiff)
pi0 = n0/(n0 + n1)
pi1 = n1/(n0 + n1)
mu0 = matrix(c(mean(k0s$ACEdiff), mean(k0s$UFEdiff)), nrow = 2)
mu1 = matrix(c(mean(k1s$ACEdiff), mean(k1s$UFEdiff)), nrow = 2)
covmat0 = cov(cbind(k0s$ACEdiff, k0s$UFEdiff))
covmat1 = cov(cbind(k1s$ACEdiff, k1s$UFEdiff))
covK = 1/(n0 + n1)*((n0-1) * covmat0 + (n1 - 1) * covmat1)
c = log(pi1/pi0) + 0.5 * t(mu1)%*%solve(covK)%*%mu1 - 0.5 * t(mu0)%*%solve(covK)%*%mu0
lhs = solve(covK)%*%(mu0-mu1)
a = lhs[1]
b = lhs[2]
```

Find that $a=`r round(a, 3)`$, $b=`r round(b,3)`$ and $c=`r round(c, 3)`$, which gives the class boundary $x_2 = `r round(c/b, 3)` + `r -round(a/b,3)` x_1$

### Part 3
```{r, eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center"}
tennis$istrain = 1
tennis[-train, ]$istrain = 0
df2 = data.frame(tennis, "Player1won"=as.factor(tennis$Result), "Istrain"=as.factor(tennis$istrain))
ggplot(df2, aes(x = ACEdiff, y = UFEdiff, color = Player1won, shape = Istrain)) + geom_abline(intercept=c/b, slope=-a/b) + geom_point(size=2) + theme_bw()
```

## f)

```{r, echo=TRUE, eval=TRUE}
lda.fit = lda(Result ~ ACEdiff + UFEdiff, data = tennisTrain)
lda.fit.p = predict(lda.fit, tennisTest)$class
confMat = table(lda.fit.p, tennisTest$Result)
confMat
```

The sensitivity is `r round(confMat[2,2]/(confMat[2,1] + confMat[2,2]), 3)` and the specificity is `r round(confMat[1,1]/(confMat[1,1] + confMat[1,2]), 3)`

## g)

```{r, echo=TRUE, eval=TRUE}
qda.fit = qda(Result ~ ACEdiff + UFEdiff, data = tennisTrain)
qda.fit.p = predict(qda.fit, tennisTest)$class
confMat = table(qda.fit.p, tennisTest$Result)
confMat
```

The sensitivity is `r round(confMat[2,2]/(confMat[2,1] + confMat[2,2]), 3)` and the specificity is `r round(confMat[1,1]/(confMat[1,1] + confMat[1,2]), 3)`

## h)


# Problem 4

## a)



## b)

## c)


```{r, eval=TRUE, echo=TRUE}
id <- "1I6dk1fA4ujBjZPo3Xj8pIfnzIa94WKcy" # google file ID
d.chd <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
fit.4c = glm(chd ~ sbp + sex, data = d.chd, family = "binomial")
summary(fit.4c)
```

## d)





