---
subtitle: "TMA4268 Statistical Learning V2020"
title: "Compulsory exercise 1: Group 13"
author: "Vemund Tjessem, Erik Andre Klepp Vik"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  #html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

```{r,eval=TRUE,echo=TRUE}
# install.packages("knitr") #probably already installed
# install.packages("rmarkdown") #probably already installed
# install.packages("ggplot2") #plotting with ggplot
# install.packages("ggfortify")  
# install.packages("MASS")  
# install.packages("dplyr")  
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
```


# Problem 1


## a)

The expected MSE for the function $\hat{f}(x_i)$ is

\begin{equation}
  MSE_{train} = \frac{1}{n} \sum_{i=1}^{n} (Y_{i}-\hat{f}(x_{i}))^2
\end{equation}{i}

Further, the expected test mean squared error (MSE) at x$_0$

\begin{equation}
  E[y_0-\hat{f}(x_i)]^2
\end{equation}

## b)

This shows that the error term can be decomposed into three terms, that is the irreducible error, the variance of prediction and the squared bias, respectively.

\begin{align}
    E[y_0-\hat{f}(x_0)]^2 &= E [(y_0 - E(\hat{f}(x_0))- \hat{f}(x_0))^2] \\
    &= [(y_0 - E(y_0))^2 + 2 ((y_0 - E(y_0))(E(y_0) - \hat{f}(x_0)) (E(y_0)-\hat{f}(x_0))^2] \\
    &= E[(y_0-E(y_0))^2] + E[(E(y_0) - \hat{f}(x_0))^2] + \epsilon \\
    &= Var(\epsilon) + Var(\hat{f}(x_0) ) + (f(x_0) - E[\hat{f}(x_0)])^2
\end{align}

## c)

\begin{itemize}
  \item Irreducible error: This term cannot be reduced regardless
  how well our statistical model fits the data.
  \item Variance of the prediction at $\hat{f}(x_0)$. Relates to the amount
  by which $\hat{f}(x_0)$ is expected to change for different training
  data. If the variance is high, there is large uncertainty
  associated with the prediction. 
  \item Squared bias. The bias gives an estimate of how much the
  prediction differs from the true mean. If the bias is low the
  model gives a prediction which is close to the true value.
  \end{itemize}

## d)

(i)   TRUE
(ii)  TRUE
(iii) FALSE
(iv)  TRUE

## e)

(i)   TRUE
(ii)  TRUE
(iii) TRUE
(iv)  FALSE

## f)

(ii) 0.17

## g)

Contour plot with $\sigma_x=1$, $\sigma_y=2$ and $\rho=0.1$. This implies best correlation with figure C.


# Problem 2

```{r, eval=TRUE, echo=TRUE}
id <- "1nLen1ckdnX4P9n8ShZeU7zbXpLc7qiwt" # google file ID
d.worm <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
head(d.worm)
str(d.worm)
attach(d.worm)
```

## a)

The worm dataset has 143 rows and 5 columns. That is, 5 variables are recorded per worm observation. Out of these, Gattung, Fangdatum are qualitative variables, and Nummer, GEWICHT and MAGENUMF are quantitative. That is, we have 2 qualitative and 3 quantitative variables. 

## b) 

```{r, fig.align='center',out.extra='angle=0', eval=TRUE, echo=TRUE}
d.worm$Gattung <- as.factor(d.worm$Gattung)
ggplot(d.worm, mapping=aes(x=MAGENUMF,y=GEWICHT,colour=Gattung)) + geom_point() + theme_bw()
```

The relationship does not look linear, therefore different log responses were tried out.

```{r, fig.align='center',out.extra='angle=0', eval=TRUE, echo=TRUE}
ggplot(d.worm, mapping=aes(x=log(MAGENUMF),y=log(GEWICHT),colour=Gattung)) + geom_point() + theme_bw()
ggplot(d.worm, mapping=aes(x=MAGENUMF,y=log(GEWICHT),colour=Gattung)) + geom_point() + theme_bw()
```

It shows that log transformations were needed to make the relationship look linear. To simplify the rest of the task, only the log of the response is used. 

## c)

Plot with linear interaction term: 

```{r, fig.align='center',out.extra='angle=0', eval=TRUE, echo=TRUE}
lm.fit <- lm(log(GEWICHT)~MAGENUMF+Gattung, data = d.worm)
summary(lm.fit)
anova(lm.fit)
```

The possible linear models are for GattungL, GattungN and GattungOc respectively

\begin{align}
  Y_{1} =&  -2.53555 + 0.71187 * MAGENUMF\\
  Y_{2} =&  -2.35754 + 0.71187 * MAGENUMF\\
  Y_{3} =& -2.62628 + 0.71187 * MAGENUMF
\end{align}

Gattung seems to have a small impact as a predictor for the models. 

## d)

```{r, fig.align='center',out.extra='angle=0', eval=TRUE, echo=TRUE}
lm.fitInteraction=lm(data=d.worm, formula = log(GEWICHT) ~ MAGENUMF*Gattung)
summary(lm.fitInteraction)
```

We can observe that an interaction term is not relevant and despite beeing a relevant predictor, can be neglected. 

## e)

```{r, fig.align='center',out.extra='angle=0', eval=TRUE, echo=TRUE}
autoplot(lm.fit)
```

For the residual plot, we see a pattern, and it looks like linearity assumption is not met, because the variance follows a pattern. If the linearity condition was met, there would not be a pattern. For the Q-Q plot it looks like the plot follows the center line, which means the normal distribution assumption is true. For the standarized and residual plots we also see that there is no covariance, supporting the theory that the fit is good. 

## f)

For the analysis in e) we have used a linear regression model, which was made under 4 assumptions: 

\begin{itemize}
\item Linearity and additivity of the relationship between dependent and independent variables:
\item Statistical independence of the errors
\item Homoscedasticity (constant variance) of the errors
\item Normality of the error distribution.
\end{itemize}

The analysis of residuals plays an important role in validating the regression model. Since the statistical tests for significance are also based on assumptions, the conclusions resulting from these significance tests are called into question if the assumptions regarding $\epsilon$ are not satisfied.

The i-th residual is the difference between the observed value of the dependent variable, $y_i$, and the value predicted by the estimated regression equation, $\hat{y}_i$. These residuals, computed from the available data, are treated as estimates of the model error, $\epsilon$. As such, they are used by statisticians to validate the assumptions concerning $\epsilon$. Good judgment and experience play key roles in residual analysis.

How to fix: Consider applying a nonlinear transformation to the dependent and/or independent variables if you can think of a transformation that seems appropriate. For example, if the data are strictly positive, the log transformation is an option. If a log transformation is applied to the dependent variable only, this is equivalent to assuming that it grows (or decays) exponentially as a function of the independent variables.  If a log transformation is applied to both the dependent variable and the independent variables, this is equivalent to assuming that the effects of the independent variables are multiplicative rather than additive in their original units. This means that, on the margin, a small percentage change in one of the independent variables induces a proportional percentage change in the expected value of the dependent variable, other things being equal.  Models of this kind are commonly used in modeling price-demand relationships, as illustrated on the beer sales example on this web site. 

## g)

(i) FALSE
(ii) FALSE
(iii) FALSE
(iv) TRUE

# Problem 3

Loading the files:
```{r, echo=TRUE, eval=TRUE}
id <- "1GNbIhjdhuwPOBr0Qz82JMkdjUVBuSoZd"
tennis <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", 
    id), header = T)
head(tennis)
```

## a)
We have
\begin{equation}
\label{eq:p_i}
p_i = \frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \beta_3x_{i3} + \beta_4 x_{i4}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}} 
\end{equation}

which gives

\begin{equation}
\label{eq:logit}
  \text{logit}(p_i) = \log\left(\frac{p_i}{1-p_i}\right)= \log\left(\frac{\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \beta_3x_{i3} + \beta_4 x_{i4}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}}}{1-\frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \beta_3x_{i3} + \beta_4 x_{i4}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}}}\right)
\end{equation}

Multiplying both the numerator and denominator in Equation \ref{eq:logit} by $1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}$ gives 

\begin{equation}
  \text{logit}(p_i) = \log\left( \frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}}{1 + e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}} - (e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}})} \right)
\end{equation}

which further results in 

\begin{equation}
\label{eq:linear}
  \text{logit}(p_i) = \log(e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}) = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}
\end{equation}

It can be seen from Equation \ref{eq:linear} that $\text{logit}(p_i)$ is a linear function of the covariates $x_{i1}$, $x_{i2}$, $x_{i3}$ and $x_{i4}$.


## b)

```{r, echo=TRUE, eval=TRUE}
r.tennis = glm(Result ~ ACE.1 + ACE.2 + UFE.1 + UFE.2, data = tennis, family = "binomial")
summary(r.tennis)
```

Since the $\beta_1$ is positive one more ace for player one would increase the probability of player 1 winning. By incerasing $x_i$ by 1 the odds ratio for $Y_i=1$ increases by $\exp(\beta_1)$.

## c)

```{r, echo=TRUE, eval=TRUE}
# make variables for difference
tennis$ACEdiff = tennis$ACE.1 - tennis$ACE.2
tennis$UFEdiff = tennis$UFE.1 - tennis$UFE.2

# divide into test and train set
n = dim(tennis)[1]
n2 = n/2
set.seed(1234)  # to reproduce the same test and train sets each time you run the code
train = sample(c(1:n), replace = F)[1:n2]
tennisTest = tennis[-train, ]
tennisTrain = tennis[train, ]
```

The code for fitting a logistic regression model is given below. 

```{r tennisTrain, echo=TRUE, eval=TRUE, cache=TRUE}
# Fitting a logistic regression model on the form Result ∼ ACEdiff + UFEdiff on the training set
fit.3c = glm(Result ~ ACEdiff + UFEdiff, data = tennisTrain, family = "binomial")
summary(fit.3c)
```

The class boundary will be where $\hat P(Y=1|\boldsymbol{x}) =0.5$. When the probability is 0.5 we have $\text{logit}(p_i)=\log(1)=0$

\begin{equation}
  0 = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2}
\end{equation}

This gives the class boundary

\begin{equation}
  x_{2} = - \frac{\beta_0}{\beta2} - \frac{\beta_1}{\beta_2}x_{1}
\end{equation}



```{r, eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center"}
cof = coef(fit.3c)
a = - cof[1]/cof[3]
b = -cof[2]/cof[3]
```
The class boundary will be $x_2 = `r round(a, 3)` + `r round(b, 3)`x_1$

Making a plot of the training observations and the class boundary.
```{r , eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center"}
df = data.frame(tennisTrain, "Player1won"=as.factor(tennisTrain$Result))
ggplot(df, aes(x = ACEdiff, y = UFEdiff, color = Player1won)) + geom_abline(intercept=a, slope=b) + geom_point(size=1) + theme_bw()
```

Making a confusion matrix

```{r, echo=TRUE, eval=TRUE}
prd = predict(fit.3c, tennisTest, type="response")
confMat = table(tennisTest$Result, prd>0.5)
confMat
```

The sensitivity is `r confMat[2,2]/(confMat[2,1] + confMat[2,2])` and the specificity is `r round(confMat[1,1]/(confMat[1,1] + confMat[1,2]), 3)`


## d)

* $\pi_k$ is the prior class probabilities $\pi_k=\text{Pr}(Y=k)$. In this case it will be the probability for player 1 winning and for player 1 losing. 

* $\boldsymbol\mu_k$ is the mean of class $k$. In this case it will be a vector with the mean of difference in aces and difference in unforced errors. 

* $\boldsymbol \Sigma$ is the covariance matrix, and in this case it is assumed equal for both classes since LDA is used. The diagonal elements will be the variance of the difference in aces the variance of the difference in unforced errors and the off-diagonal elements will be the covariance.

* $f_k(\boldsymbol x)$ is the probability density function for $\boldsymbol X$ in class $k$, and is here assumed to be multivariate normally distributed with mean $\boldsymbol \mu_k$ and covariance $\boldsymbol \Sigma$.


## e)

### Part 1

\begin{equation}
P(Y=0 | {\bf X}={\boldsymbol x}) = P(Y=1 | {\bf X}={\boldsymbol x})
\end{equation}

The first step is to insert for the probability and the probability distribution.

\begin{equation}
\frac{\pi_0}{\sum_{l=1}^K \pi_l f_l({\boldsymbol x})2 \pi|\boldsymbol{\Sigma}|^{1/2}}e^{-\frac{1}{2}({\boldsymbol x}-\boldsymbol{\mu_0})^T \boldsymbol{\Sigma}^{-1}({\boldsymbol x}-\boldsymbol{\mu_0})} = \frac{\pi_1}{\sum_{l=1}^K \pi_l f_l({\boldsymbol x})2 \pi|\boldsymbol{\Sigma}|^{1/2}}e^{-\frac{1}{2}({\boldsymbol x}-\boldsymbol{\mu_1})^T \boldsymbol{\Sigma}^{-1}({\boldsymbol x}-\boldsymbol{\mu_1})}
\end{equation}

Next is taking the logarithm on both sides, which gives

\begin{align}
&\log(\pi_0)-\frac{1}{2}\boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol x + \boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0 + \boldsymbol \mu_0^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0 - \log\left(\sum_{l=1}^K \pi_l f_l({\boldsymbol x})2 \pi|\boldsymbol{\Sigma}|^{1/2}\right) = \\ &\log(\pi_1)-\frac{1}{2}\boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol x + \boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1 + \boldsymbol \mu_1^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1 - \log\left(\sum_{l=1}^K \pi_l f_l({\boldsymbol x})2 \pi|\boldsymbol{\Sigma}|^{1/2}\right)
\end{align}

After removing the terms not depending on $k$, which are equal on both sides

\begin{equation}
\label{eq:deltalikdelta}
\log(\pi_0) + \boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0 + \frac{1}{2} \boldsymbol \mu_0^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0 = \log(\pi_1) + \boldsymbol x^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1 + \frac{1}{2} \boldsymbol \mu_1^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1
\end{equation}

which equals

\begin{equation}
\delta_0({\boldsymbol x}) = \delta_1({\boldsymbol x})
\end{equation}

### Part 2

The class boundary will be the values for $\boldsymbol x$ where $\delta_0({\boldsymbol x}) = \delta_1({\boldsymbol x})$, hence Equation \ref{eq:deltalikdelta} can be used to find the class boundary.


\begin{equation}
\boldsymbol x^T \boldsymbol \Sigma^{-1}  (\boldsymbol\mu_0 - \boldsymbol\mu_1) = \log\left(\frac{\pi_1}{\pi_0}\right) + \frac{1}{2} \boldsymbol \mu_1^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_1 - \frac{1}{2} \boldsymbol \mu_0^T \boldsymbol \Sigma^{-1} \boldsymbol \mu_0
\end{equation}

Doing the matrix multiplications here will result in a equation on the form $ax_1 + bx_2=c$, which can be used to find the class boundary on the form $x_2 = \frac{c}{b} - \frac{a}{b}x_1$.

Since $\pi_k$, $\boldsymbol \mu_k$ and $\boldsymbol \Sigma$ are unknown, they have to be estimated. The estimators used are 

* $\hat{\pi}_k = \frac{n_k}{n}$
* $\hat{\boldsymbol{\mu}}_k = \frac{1}{n_k}\sum_{i:y_i=k} {\bf X}_i$
* $\hat{\boldsymbol{\Sigma}}_k=\frac{1}{n_k-1}\sum_{i:y_i=k} ({\bf X}_i-\hat{\boldsymbol{\mu}}_k ) ({\bf X}_i-\hat{\boldsymbol{\mu}}_k)^T$
* $\hat{\boldsymbol{\Sigma}}= \sum_{k=1}^K \frac{n_k - 1}{n - K} \cdot \hat{\boldsymbol{\Sigma}}_k$

```{r, eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center", cache=TRUE}
k0s = subset(tennisTrain, tennisTrain$Result < 0.5)
k1s = subset(tennisTrain, tennisTrain$Result > 0.5)
n0 = length(k0s$UFEdiff)
n1 = length(k1s$UFEdiff)
pi0 = n0/(n0 + n1)
pi1 = n1/(n0 + n1)
mu0 = matrix(c(mean(k0s$ACEdiff), mean(k0s$UFEdiff)), nrow = 2)
mu1 = matrix(c(mean(k1s$ACEdiff), mean(k1s$UFEdiff)), nrow = 2)
mu0
mu1
covmat0 = cov(cbind(k0s$ACEdiff, k0s$UFEdiff))
covmat1 = cov(cbind(k1s$ACEdiff, k1s$UFEdiff))
covK = 1/(n0 + n1)*((n0-1) * covmat0 + (n1 - 1) * covmat1)
c = log(pi1/pi0) + 0.5 * t(mu1)%*%solve(covK)%*%mu1 - 0.5 * t(mu0)%*%solve(covK)%*%mu0
lhs = solve(covK)%*%(mu0-mu1)
a = lhs[1]
b = lhs[2]
```

Find that $a=`r round(a, 3)`$, $b=`r round(b,3)`$ and $c=`r round(c, 3)`$, which gives the class boundary $x_2 = `r round(c/b, 3)` + `r -round(a/b,3)` x_1$

### Part 3
```{r, eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center", cache=TRUE}
tennis$istrain = 1
tennis[-train, ]$istrain = 0
df2 = data.frame(tennis, "Player1won"=as.factor(tennis$Result), "Istrain"=as.factor(tennis$istrain))
ggplot(df2, aes(x = ACEdiff, y = UFEdiff, color = Player1won, shape = Istrain)) + geom_abline(intercept=c/b, slope=-a/b) + geom_point(size=1) + theme_bw()
```



## f)

```{r, echo=TRUE, eval=TRUE}
lda.fit = lda(Result ~ ACEdiff + UFEdiff, data = tennisTrain)
lda.fit.p = predict(lda.fit, tennisTest)$class
confMat = table(lda.fit.p, tennisTest$Result)
confMat
```

The sensitivity is `r round(confMat[2,2]/(confMat[2,1] + confMat[2,2]), 3)` and the specificity is `r round(confMat[1,1]/(confMat[1,1] + confMat[1,2]), 3)`


## g)

```{r, echo=TRUE, eval=TRUE}
qda.fit = qda(Result ~ ACEdiff + UFEdiff, data = tennisTrain)
qda.fit.p = predict(qda.fit, tennisTest)$class
confMat = table(qda.fit.p, tennisTest$Result)
confMat
```

The sensitivity is `r round(confMat[2,2]/(confMat[2,1] + confMat[2,2]), 3)` and the specificity is `r round(confMat[1,1]/(confMat[1,1] + confMat[1,2]), 3)`

## h)

Looking at the confusion matrices, QDA has the lowest sensitivity and the specificity is lower than for LDA, so QDA should not be used. Looking at the plot of the QDA decision boundary we see that to the left in the plot around `UFEdiff`=0 and below QDA classifies as win while glm and LDA would classify as loss, this may be why QDA is worse than the other two. glm has the highest sensitivity and the lowest specificity while LDA has better sensitivity than QDA and the best spesificity. Summing up the two values glm has the highest value, so that is what should be used.


# Problem 4

## a)

Given a set of values for $K$, 10-fold cross validation is performed by first randomly dividing the data into a training set and a testing set, the testing set is not used until the very end. The training dataset is then randomly divided into 10 more or less equal parts, $C_1$, $C_2$, ..., $C_{10}$. $C_k$ denotes the indices of the observations in part $k$. 9 parts are used for training the model and 1 is used for testing the model. This is done 10 times with a new set used as test set each time. The error is then calculated using the loss function in Equation \ref{eq:10foldCV}.

\begin{equation}
\label{eq:10foldCV}
\text{CV}_{10} = \sum_{k=1}^{10} \frac{n_k}{10} \text{Err}_k
\end{equation}

where $n_k$ is the number of observations in part $k$. The error for part $k$ is

\begin{equation}
\text{Err}_k  = \sum_{i\in C_k} \frac{\text{I}(y_i\neq \hat y_i)}{n_k}
\end{equation}

where $\text{I}$ is the indicator function defined as 

\begin{equation}
\text{I}(a\neq\hat{a}) = \begin{cases} 1 \text{ if } a \neq \hat{a} \\ 
0 \text{ else } \end{cases}
\end{equation}

This is done for each value of $K$ we want to consider. This will result in a plot of  $\text{CV}_{10}$ against $K$. Based on this plot the best model can be selected. The best model will typically be the one with the lowest $\text{CV}_{10}$. The model is then fit using the whole training dataset, and tested using the test set which has not been used yet. 


## b)

(i) TRUE
(ii) TRUE
(iii) FALSE
(iv) FALSE

## c)


```{r, eval=TRUE, echo=TRUE}
id <- "1I6dk1fA4ujBjZPo3Xj8pIfnzIa94WKcy" # google file ID
d.chd <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
fit.4c = glm(chd ~ sbp + sex, data = d.chd, family = "binomial")
summary(fit.4c)
eta <- summary(fit.4c)$coef[, 1] %*% c(1, 140, 1)
pchd = exp(eta)/(1 + exp(eta))
```

The probability of a male with $sbp=140$ having coronary heart disease is `r round(pchd, 3)`.

```{r, eval=FALSE, echo=FALSE, fig.width=6,fig.height=4,fig.align="center"}
ggplot(d.chd, aes(x = sbp, y = sex, color=as.factor(chd))) + geom_point(size=1) + theme_bw()
```


## d)

Will now perform bootstrapping with $B=1000$. For each iteration the model is fit with the resampled data and is used to create a bootstrap estimate of the probability for `chd`, given `sbp`=140 and `sex`=male, $p$. The bootstrap estimates, $\hat{p}_i$, are stored in the `estimator` variable. 

```{r, eval=TRUE, echo=TRUE, fig.width=6,fig.height=4,fig.align="center", cache=TRUE}
B = 1000
n = dim(d.chd)[1]
estimator = rep(NA, B)
for (b in 1:B) {
  newind = sample(x = c(1:n), size = n, replace = TRUE)
  newsample = d.chd[newind,1:3]
  fit.4d = glm(chd ~ sbp + sex, data = newsample, family = "binomial")
  eterm <- summary(fit.4d)$coef[, 1] %*% c(1, 140, 1)
  estimator[b] = exp(eterm)/(1 + exp(eterm))
}
Mxb = mean(estimator)
SE = sqrt(1/(B-1) * sum((estimator-Mxb)^2))
confinterval = quantile(estimator, probs = c(2.5, 97.5)/100)
```

Now that there are $1000$ different values for the probability the standard error can be calculated. The mean is simply

\begin{equation}
\bar{\hat{p}} = \frac{1}{B}\sum_{i=1}^B \hat{p}_{i}
\end{equation}

The standard error is then

\begin{equation}
\text{SE}_B(\hat{p}) = \sqrt{\frac{1}{B-1}\sum_{i=1}^B (\hat{p}_{i}-\bar{\hat{p}})^2} = `r SE`
\end{equation}


The standard error is `r round(SE, 4)`. The confidence interval for $\hat p$ is (`r round(confinterval, 3)`)